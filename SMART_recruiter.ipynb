{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gTTS in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: click in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gTTS) (7.1.2)\n",
      "Requirement already satisfied: requests in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gTTS) (2.26.0)\n",
      "Requirement already satisfied: six in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gTTS) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->gTTS) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->gTTS) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->gTTS) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->gTTS) (2021.10.8)\n",
      "Requirement already satisfied: playsound in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: pyttsx3 in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.90)\n",
      "Requirement already satisfied: pypiwin32 in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyttsx3) (302)\n",
      "Requirement already satisfied: comtypes in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyttsx3) (1.1.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: speechRecognition in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pydub in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: flashtext in c:\\users\\iamro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.7)\n"
     ]
    }
   ],
   "source": [
    "#installed libraries\n",
    "\n",
    "!pip install gTTS\n",
    "!pip install playsound  \n",
    "!pip install pyttsx3  \n",
    "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
    "!pip install speechRecognition\n",
    "!pip install pydub\n",
    "!pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iamro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import gtts  \n",
    "from playsound import playsound  \n",
    "import pyttsx3\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "import pyaudio\n",
    "import wave\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio\n",
    "from gtts import gTTS\n",
    "import time\n",
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pydub import AudioSegment\n",
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_speech(mytext):\n",
    "    language = 'en'\n",
    "    myobj = gTTS(text=mytext, lang=language, slow=False)\n",
    "    myobj.save(\"chat_speech.mp3\")\n",
    "    os.system(\"chat_speech.mp3\")\n",
    "    print(\"RECRUITER:\",mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_speech():\n",
    "    CHUNK = 1024 \n",
    "    FORMAT = pyaudio.paInt16 #paInt8\n",
    "    CHANNELS = 2 \n",
    "    RATE = 44100 #sample rate\n",
    "    RECORD_SECONDS = 10\n",
    "    WAVE_OUTPUT_FILENAME = \"input.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK) #buffer\n",
    "\n",
    "    print(\"* recording\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data) # 2 bytes(16 bits) per channel\n",
    "\n",
    "    print(\"* done recording\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    sound = AudioSegment.from_wav(\"input.wav\")\n",
    "    sound = sound.set_channels(1)\n",
    "    sound.export(\"input.wav\", format=\"wav\")\n",
    "    r = sr.Recognizer()\n",
    "    with sr.WavFile(\"input.wav\") as source:\n",
    "        audio_data = r.listen(source)\n",
    "        # convert speech to text\n",
    "        text = r.recognize_google(audio_data)\n",
    "        print(\"ME:\",text)\n",
    "    sentiment = predict_sentiment(text)\n",
    "    return audio_data, predict_emo('input.wav')-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_info(P):\n",
    "    place_d = {'Hyderabad':\"it's famous for it's biryani !\" }\n",
    "    return place_d[P]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype = \"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised' }\n",
    "\n",
    "observed_emotions = ['calm', 'happy', 'fearful', 'disgust']\n",
    "def load_data(test_size=0.2):\n",
    "    x,y = [],[]\n",
    "    for file in glob.glob(\"D:\\datasets\\SER\\Actor_*\\\\*.wav\"):\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300, 200, 100),\n",
       "              learning_rate='adaptive', max_iter=500)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, \n",
    "                      hidden_layer_sizes=(300,200,100), learning_rate='adaptive', max_iter=500)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300, 200, 100),\n",
       "              learning_rate='adaptive', max_iter=500)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.52%\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emo(file):\n",
    "    x = []\n",
    "    feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "    x.append(feature)\n",
    "    np.array(x)\n",
    "    l = model.predict(x)\n",
    "    s = ''\n",
    "    s.join(l)\n",
    "    dec = -100\n",
    "    if s in ['sad','fearful']:\n",
    "        dec = 100\n",
    "    if s in ['disgust','angry']:\n",
    "        dec = 200\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it's famous for it's biryani !\""
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "place_info(\"Hyderabad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_12680/3215681510.py, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\iamro\\AppData\\Local\\Temp/ipykernel_12680/3215681510.py\"\u001b[1;36m, line \u001b[1;32m53\u001b[0m\n\u001b[1;33m    prev_comp =\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def personal_round():\n",
    "    r = sr.Recognizer()\n",
    "    points_scored = 1000 #points keep decreasing based on answers by the candidate\n",
    "    \n",
    "    #question 1\n",
    "    mytext = \"Hello, let's start with some personal details about you. What is your name, where are you from and where did u work earlier?\"\n",
    "    language = 'en'\n",
    "    output_speech(mytext)\n",
    "    time.sleep(11)\n",
    "    audio_data, emote_points = input_speech()  \n",
    "    points_scored -= int(emote_points)\n",
    "    text1 = NER(r.recognize_google(audio_data))\n",
    "    for word in text1.ents:\n",
    "        if word.label_==\"PERSON\":\n",
    "            name=word.text\n",
    "        if word.label_==\"GPE\":\n",
    "            place=word.text\n",
    "        if word.label_==\"ORG\":\n",
    "            prev=word.text\n",
    "    N=\"Rishabh\"        \n",
    "    mytext = \"Hi \" + N + \", all the best for your interview!\"\n",
    "    output_speech(mytext)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    mytext = N + \", What are some of your strengths?\"\n",
    "    output_speech(mytext)\n",
    "    time.sleep(6)\n",
    "    audio_data, emote_points = input_speech()\n",
    "    points_scored += emote_points\n",
    "    strenghts = ['enthusiasm','enthusiastic','trustworthiness','trustworthy','creativity','creative','discipline','disciplined','patience','patient','respectfulness','respectfull','determination','determined','dedication','dedicated','honesty','honest','versatility','versatile','liveliness','lively','hard working']\n",
    "    keywordprocessor = KeywordProcessor(case_sensitive=False)\n",
    "    keywordprocessor.add_keywords_from_list(strenghts)\n",
    "    Extractedkeywords = keywordprocessor.extract_keywords(r.recognize_google(audio_data))\n",
    "    points_scored += len(keywordprocessor)*100\n",
    "    \n",
    "    mytext = N + \", What are some of your weaknesses?\"\n",
    "    output_speech(mytext)\n",
    "    time.sleep(6)\n",
    "    audio_data, emote_points = input_speech()\n",
    "    points_scored += emote_points\n",
    "    keywordprocessor = KeywordProcessor(case_sensitive=False)\n",
    "    keywordprocessor.add_keywords_from_list(strenghts)\n",
    "    Extractedkeywords = keywordprocessor.extract_keywords(r.recognize_google(audio_data))\n",
    "    points_scored -= len(keywordprocessor)*50\n",
    "    \n",
    "    P = \"Hyderabad\"\n",
    "    mytext = \"Tell me about \" + P + \",how is it there and what is \" + P + \"famous for.\" + \" Heard \" + place_info(place)\n",
    "    output_speech(mytext)\n",
    "    time.sleep(6)\n",
    "    audio_data, emote_points = input_speech()\n",
    "    points_scored += emote_points\n",
    "    \n",
    "    prev = \"Samsung\"\n",
    "    mytext = \"Why did you leave from \" + prev\n",
    "    output_speech(mytext)\n",
    "    time.sleep(6)\n",
    "    audio_data, emote_points = input_speech()\n",
    "    points_scored += emote_points\n",
    "    \n",
    "    mytext = \"Your score till now is\" + str(points_scored)\n",
    "    output_speech(mytext)\n",
    "    time.sleep(6)\n",
    "    \n",
    "    mytext = \"Let's move to some technical questions!\"\n",
    "    output_speech(mytext)\n",
    "    time.sleep(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resume summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def technical_round(points_scored):\n",
    "    \n",
    "    \n",
    "    #question 1\n",
    "    mytext = \"What is a doubly linked list and specify its applications?\"\n",
    "    output_speech(mytext)\n",
    "    time.sleep(8)\n",
    "    audio_data, emote_points = input_speech()\n",
    "    points_scored += emote_points\n",
    "    #answer 1\n",
    "    answer = \"This is a complex type of a linked list wherein a node has two references: One that connects to the next node in the sequence Another that connects to the previous node. This structure allows traversal of the data elements in both directions (left to right and vice versa). Applications of DLL are: A music playlist with next song and previous song navigation options. The browser cache with BACK-FORWARD visited pages The undo and redo functionality on platforms such as word, paint etc, where you can reverse the node to get to the previous page.\"\n",
    "    \n",
    "    #question 2\n",
    "    mytext = \"What is a priority queue?\"\n",
    "    output_speech(mytext)\n",
    "    time.sleep(8)\n",
    "    audio_data, emote_points = input_speech()\n",
    "    points_scored += emote_points\n",
    "    #answer 2\n",
    "    answer = \" A priority queue is an abstract data type that is like a normal queue but has priority assigned to elements. Elements with higher priority are processed before the elements with a lower priority. In order to implement this, a minimum of two queues are required - one for the data and the other to store the priority.\"\n",
    "    \n",
    "    #question 3\n",
    "    mytext = \"What is a AVL tree?\"\n",
    "    output_speech(mytext)\n",
    "    time.sleep(8)\n",
    "    audio_data, emote_points = input_speech()\n",
    "    points_scored += emote_points\n",
    "    #answer 3\n",
    "    answer = \"AVL trees are height balancing BST. AVL tree checks the height of left and right sub-trees and assures that the difference is not more than 1. This difference is called Balance Factor and is calculates as. BalanceFactor = height(left subtree) − height(right subtree)\"\n",
    "    \n",
    "    #question 4\n",
    "    mytext = \"What is a heap?\"\n",
    "    output_speech(mytext)\n",
    "    time.sleep(8)\n",
    "    audio_data, emote_points = input_speech()\n",
    "    points_scored += emote_points\n",
    "    #answer 4\n",
    "    answer = \"Heap is a special tree-based non-linear data structure in which the tree is a complete binary tree. A binary tree is said to be complete if all levels are completely filled except possibly the last level and the last level has all elements towards as left as possible.\"\n",
    "    \n",
    "    #question 5\n",
    "    mytext = \"What is the difference between BFS and DFS?\"\n",
    "    output_speech(mytext)\n",
    "    time.sleep(8)\n",
    "    audio_data, emote_points = input_speech()\n",
    "    points_scored += emote_points\n",
    "    #answer 5\n",
    "    answer = \"BFS and DFS both are the traversing methods for a graph. Graph traversal is nothing but the process of visiting all the nodes of the graph. The main difference between BFS and DFS is that BFS traverses level by level whereas DFS follows first a path from the starting to the end node, then another path from the start to end, and so on until all nodes are visited. Furthermore, BFS uses queue data structure for storing the nodes whereas DFS uses the stack for traversal of the nodes for implementation. DFS yields deeper solutions that are not optimal, but it works well when the solution is dense whereas the solutions of BFS are optimal. You can learn more about BFS here: Breadth First Search and DFS here: Depth First Search.\"\n",
    "    \n",
    "    #question 6\n",
    "    mytext = \"What is your work experience such as internship and freelancing?\"\n",
    "    output_speech(mytext)\n",
    "    time.sleep(8)\n",
    "    audio_data, emote_points = input_speech()\n",
    "    points_scored += emote_points\n",
    "    #answer 5\n",
    "    companies_pos = [\"apple\",\"samsung\",\"foxconn\",\"alphabet\", \"microsoft\" ,\"huawei\",\"dell\",\"hitachi\", \"amazon\",\"flipkart\",\"atlassian\",\"google\",\"oracle\",\"fiverr\",\"fiver\"]\n",
    "    companies_neg = [\"no\",\"sorry\",\"not\",\"couldn't\"]\n",
    "    keywordprocessor = KeywordProcessor(case_sensitive=False)\n",
    "    keywordprocessor.add_keywords_from_list(companies_pos)\n",
    "    Extractedkeywords = keywordprocessor.extract_keywords(r.recognize_google(audio_data))\n",
    "    points_scored += len(keywordprocessor)*50\n",
    "    keywordprocessor = KeywordProcessor(case_sensitive=False)\n",
    "    keywordprocessor.add_keywords_from_list(companies_neg)\n",
    "    Extractedkeywords = keywordprocessor.extract_keywords(r.recognize_google(audio_data))\n",
    "    points_scored -= len(keywordprocessor)*50\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "#from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2339a9b08b</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16fab9f95b</td>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "      <td>like</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>74a76f6e0a</td>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>DANGERously</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>04dd1d2e34</td>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "      <td>lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bbe3cbf620</td>\n",
       "      <td>test test from the LG enV2</td>\n",
       "      <td>test test from the LG enV2</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        textID                                               text  \\\n",
       "0   cb774db0d1                I`d have responded, if I were going   \n",
       "1   549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2   088c60f138                          my boss is bullying me...   \n",
       "3   9642c003ef                     what interview! leave me alone   \n",
       "4   358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5   28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6   6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7   50e14c0bb8                                         Soooo high   \n",
       "8   e050245fbd                                        Both of you   \n",
       "9   fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "10  2339a9b08b   as much as i love to be hopeful, i reckon the...   \n",
       "11  16fab9f95b  I really really like the song Love Story by Ta...   \n",
       "12  74a76f6e0a       My Sharpie is running DANGERously low on ink   \n",
       "13  04dd1d2e34  i want to go to music tonight but i lost my vo...   \n",
       "14  bbe3cbf620                         test test from the LG enV2   \n",
       "\n",
       "                                        selected_text sentiment  \n",
       "0                 I`d have responded, if I were going   neutral  \n",
       "1                                            Sooo SAD  negative  \n",
       "2                                         bullying me  negative  \n",
       "3                                      leave me alone  negative  \n",
       "4                                       Sons of ****,  negative  \n",
       "5   http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                 fun  positive  \n",
       "7                                          Soooo high   neutral  \n",
       "8                                         Both of you   neutral  \n",
       "9                        Wow... u just became cooler.  positive  \n",
       "10  as much as i love to be hopeful, i reckon the ...   neutral  \n",
       "11                                               like  positive  \n",
       "12                                        DANGERously  negative  \n",
       "13                                               lost  negative  \n",
       "14                         test test from the LG enV2   neutral  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('D:/datasets/sentiment analysis/train.csv')\n",
    "train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['selected_text','sentiment']]\n",
    "train[\"selected_text\"].fillna(\"No content\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depure_data(data):\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(\"\\'\", \"\", data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I`d have responded, if I were going',\n",
       " 'Sooo SAD',\n",
       " 'bullying me',\n",
       " 'leave me alone',\n",
       " 'Sons of ****,']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "#Splitting pd.Series to list\n",
    "data_to_list = train['selected_text'].values.tolist()\n",
    "for i in range(len(data_to_list)):\n",
    "    temp.append(depure_data(data_to_list[i]))\n",
    "list(temp[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['have', 'responded', 'if', 'were', 'going'], ['sooo', 'sad'], ['bullying', 'me'], ['leave', 'me', 'alone'], ['sons', 'of'], ['some', 'shameless', 'plugging', 'for', 'the', 'best', 'rangers', 'forum', 'on', 'earth'], ['fun'], ['soooo', 'high'], ['both', 'of', 'you'], ['wow', 'just', 'became', 'cooler']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) \n",
    "        # deacc=True removes punctuations\n",
    "        \n",
    "\n",
    "data_words = list(sent_to_words(temp))\n",
    "\n",
    "print(data_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have responded if were going', 'sooo sad', 'bullying me', 'leave me alone', 'sons of']\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(len(data_words)):\n",
    "    data.append(detokenize(data_words[i]))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(train['sentiment'])\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 'neutral':\n",
    "        y.append(0)\n",
    "    if labels[i] == 'negative':\n",
    "        y.append(1)\n",
    "    if labels[i] == 'positive':\n",
    "        y.append(2)\n",
    "y = np.array(y)\n",
    "labels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   68  146   41]\n",
      " [   0    0    0 ...    0  397   65]\n",
      " [   0    0    0 ...    0    0   11]\n",
      " ...\n",
      " [   0    0    0 ...  372   10    3]\n",
      " [   0    0    0 ...   24  542    4]\n",
      " [   0    0    0 ... 2424  199  657]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "max_words = 5000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "tweets = pad_sequences(sequences, maxlen=max_len)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20610 6871 20610 6871\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets,labels, random_state=0)\n",
    "print (len(X_train),len(X_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/5\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.7929 - accuracy: 0.6482 E\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.73294, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 68s 99ms/step - loss: 0.7929 - accuracy: 0.6482 - val_loss: 0.6404 - val_accuracy: 0.7329\n",
      "Epoch 2/5\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.7676\n",
      "Epoch 00002: val_accuracy improved from 0.73294 to 0.79101, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 58s 90ms/step - loss: 0.5803 - accuracy: 0.7676 - val_loss: 0.5306 - val_accuracy: 0.7910\n",
      "Epoch 3/5\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.7985\n",
      "Epoch 00003: val_accuracy improved from 0.79101 to 0.80512, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 61s 95ms/step - loss: 0.5094 - accuracy: 0.7985 - val_loss: 0.5066 - val_accuracy: 0.8051\n",
      "Epoch 4/5\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.8195\n",
      "Epoch 00004: val_accuracy improved from 0.80512 to 0.81153, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 62s 96ms/step - loss: 0.4741 - accuracy: 0.8195 - val_loss: 0.4912 - val_accuracy: 0.8115\n",
      "Epoch 5/5\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4533 - accuracy: 0.8274\n",
      "Epoch 00005: val_accuracy improved from 0.81153 to 0.81895, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 62s 96ms/step - loss: 0.4533 - accuracy: 0.8274 - val_loss: 0.4681 - val_accuracy: 0.8189\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
    "model2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
    "model2.add(layers.Dense(3,activation='softmax'))\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model2.fit(X_train, y_train, epochs=5,validation_data=(X_test, y_test),callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 - 5s - loss: 0.4681 - accuracy: 0.8189 - 5s/epoch - 24ms/step\n",
      "Model accuracy:  0.8189492225646973\n"
     ]
    }
   ],
   "source": [
    "best_model = keras.models.load_model(\"best_model2.hdf5\")\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Model accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00522882, 0.01172866, 0.9830425 ],\n",
       "       [0.017721  , 0.9753108 , 0.00696822],\n",
       "       [0.5261967 , 0.01086186, 0.46294138],\n",
       "       ...,\n",
       "       [0.03248564, 0.9484236 , 0.01909068],\n",
       "       [0.92924863, 0.02571947, 0.0450319 ],\n",
       "       [0.55078554, 0.41869769, 0.03051668]], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6871, 200)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(x):\n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    x = url_pattern.sub(r'', x)\n",
    "    # Remove Emails\n",
    "    x = re.sub('\\S*@\\S*\\s?', '', x)\n",
    "    # Remove new line characters\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "    # Remove distracting single quotes\n",
    "    x = re.sub(\"\\'\", \"\", x)\n",
    "    \n",
    "    \n",
    "    temp=[]\n",
    "    temp.append(x)\n",
    "    data_words = list(sent_to_words(temp))\n",
    "    \n",
    "    data = []\n",
    "    for i in range(len(data_words)):\n",
    "        data.append(detokenize(data_words[i]))\n",
    "    \n",
    "    data = np.array(data)\n",
    "    max_words = 5000\n",
    "    max_len = 200\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    sequences = tokenizer.texts_to_sequences(data)\n",
    "    num_val = pad_sequences(sequences, maxlen=max_len)\n",
    "    \n",
    "    y = best_model.predict(num_val)\n",
    "    if max(y[0]) == y[0][0]:\n",
    "        return 25\n",
    "    if max(y[0]) == y[0][1]:\n",
    "        return -50\n",
    "    if max(y[0]) == y[0][2]:\n",
    "        return 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"I`d have responded, if I were going\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"Obama speaks to the media in Illinois\"\n",
    "sentence_2 = \"President greets the press in Chicago\"\n",
    "sentence_3 = \"Nokia is my favorite company\"\n",
    "\n",
    "s1='my name is rishabh'\n",
    "s2='rishabh is the name'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_round()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
